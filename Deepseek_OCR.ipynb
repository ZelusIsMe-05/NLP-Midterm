{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3qSfxbc1t4t"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPHfaHf91t4w"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tocz_dYD1t4x"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaXl4v_P1t4x"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFsDdpz71t4x"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/dataset.zip\" -d \"/content\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "WMOzY4kb4G0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyim9ave1t4z"
      },
      "source": [
        "Let's prepare the OCR model to our local first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmgJtpdz1t4z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E9HVoqSZArnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n",
        "    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"./deepseek_ocr\",\n",
        "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code=True,\n",
        "    unsloth_force_compile=True,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZVh2ByxFNh"
      },
      "source": [
        "### Let's Evaluate Deepseek-OCR Baseline Performance on Persian Transcription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"/content/dataset\")\n",
        "# dataset = load_dataset(\"hezarai/parsynth-ocr-200k\", split = \"train[:2000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZHjLlxrI_TH"
      },
      "outputs": [],
      "source": [
        "# Save an image that will not be used during training for evaluation purposes\n",
        "dataset[\"test\"][0][\"image\"].save(\"0000_samples.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68O9GTfTXfHo"
      },
      "outputs": [],
      "source": [
        "dataset[\"test\"][0][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpS8-2lHCEOJ"
      },
      "outputs": [],
      "source": [
        "# prompt = \"<image>\\nFree OCR. \"\n",
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = '0000_samples.png'\n",
        "output_path = 'your/output/dir'\n",
        "# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6LYHC3F0Ihf"
      },
      "source": [
        "<h3>Baseline Model Performance: 23% Character Error Rate (CER) for this sample !</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzckMII_02s_"
      },
      "source": [
        "# Let's finetune Deepseek-OCR !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We'll be using a dataset for Persian OCR. The goal is to convert these images into a computer readable form - ie text. This can be very useful for digitizing Persian text.\n",
        "\n",
        "You can access the dataset [here](https://huggingface.co/datasets/hezarai/parsynth-ocr-200k).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"<|User|>\",\n",
        "  \"content\": \"\",\n",
        "  \"images\": []\n",
        "},\n",
        "{ \"role\": \"<|Assistant|>\",\n",
        "  \"content\": \"\"\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json, os\n",
        "\n",
        "DATA_ROOT = \"/content/dataset\"\n",
        "\n",
        "def load_split(json_path, image_dir):\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    samples = []\n",
        "    for img_name, text in data.items():\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        samples.append({\n",
        "            \"image\": img,\n",
        "            \"text\": text,\n",
        "        })\n",
        "    return Dataset.from_list(samples)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": load_split(f\"{DATA_ROOT}/train.json\", f\"{DATA_ROOT}/images\"),\n",
        "    \"validation\": load_split(f\"{DATA_ROOT}/val.json\", f\"{DATA_ROOT}/images\"),\n",
        "    \"test\": load_split(f\"{DATA_ROOT}/test.json\", f\"{DATA_ROOT}/test\"),\n",
        "})\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "2okYGGo9JYze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"<image>\\nFree OCR.\"\n",
        "\n",
        "def convert_to_conversation(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"<|User|>\",\n",
        "                \"content\": instruction,\n",
        "                \"images\": [sample[\"image\"]],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"<|Assistant|>\",\n",
        "                \"content\": sample[\"text\"],\n",
        "            },\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# dataset = dataset.map(\n",
        "#     convert_to_conversation,\n",
        "#     remove_columns=dataset[\"train\"].column_names,\n",
        "# )\n",
        "\n",
        "train_data = [convert_to_conversation(sample) for sample in dataset[\"train\"]]\n",
        "val_data   = [convert_to_conversation(sample) for sample in dataset[\"validation\"]]\n"
      ],
      "metadata": {
        "id": "_asfyogiQjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "KIaDdOpTXfdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-9u-OD6_gE"
      },
      "source": [
        "Let's convert the dataset into the \"correct\" format for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLHM0PGn30x5"
      },
      "outputs": [],
      "source": [
        "# converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for the first example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "# converted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E2WR-p20LcG_"
      },
      "outputs": [],
      "source": [
        "# @title Create datacollator\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from PIL import Image, ImageOps\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import io\n",
        "\n",
        "from deepseek_ocr.modeling_deepseekocr import (\n",
        "    format_messages,\n",
        "    text_encode,\n",
        "    BasicImageTransform,\n",
        "    dynamic_preprocess,\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class DeepSeekOCRDataCollator:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tokenizer: Tokenizer\n",
        "        model: Model\n",
        "        image_size: Size for image patches (default: 640)\n",
        "        base_size: Size for global view (default: 1024)\n",
        "        crop_mode: Whether to use dynamic cropping for large images\n",
        "        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n",
        "    \"\"\"\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    image_size: int = 640\n",
        "    base_size: int = 1024\n",
        "    crop_mode: bool = True\n",
        "    image_token_id: int = 128815\n",
        "    train_on_responses_only: bool = True\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_size: int = 640,\n",
        "        base_size: int = 1024,\n",
        "        crop_mode: bool = True,\n",
        "        train_on_responses_only: bool = True,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.crop_mode = crop_mode\n",
        "        self.image_token_id = 128815\n",
        "        self.dtype = model.dtype  # Get dtype from model\n",
        "        self.train_on_responses_only = train_on_responses_only\n",
        "\n",
        "        self.image_transform = BasicImageTransform(\n",
        "            mean=(0.5, 0.5, 0.5),\n",
        "            std=(0.5, 0.5, 0.5),\n",
        "            normalize=True\n",
        "        )\n",
        "        self.patch_size = 16\n",
        "        self.downsample_ratio = 4\n",
        "\n",
        "        # Get BOS token ID from tokenizer\n",
        "        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n",
        "            self.bos_id = tokenizer.bos_token_id\n",
        "        else:\n",
        "            self.bos_id = 0\n",
        "            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n",
        "\n",
        "    def deserialize_image(self, image_data) -> Image.Image:\n",
        "        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n",
        "        if isinstance(image_data, Image.Image):\n",
        "            return image_data.convert(\"RGB\")\n",
        "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "            image_bytes = image_data['bytes']\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            return image.convert(\"RGB\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
        "\n",
        "    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n",
        "        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n",
        "        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "        width_crop_num, height_crop_num = crop_ratio\n",
        "\n",
        "        if self.crop_mode:\n",
        "            img_tokens = num_queries_base * num_queries_base + 1\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                img_tokens += (num_queries * width_crop_num + 1) * (num_queries * height_crop_num)\n",
        "        else:\n",
        "            img_tokens = num_queries * num_queries + 1\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Process a single image based on crop_mode and size thresholds\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n",
        "        \"\"\"\n",
        "        images_list = []\n",
        "        images_crop_list = []\n",
        "        images_spatial_crop = []\n",
        "\n",
        "        if self.crop_mode:\n",
        "            # Determine crop ratio based on image size\n",
        "            if image.size[0] <= 640 and image.size[1] <= 640:\n",
        "                crop_ratio = (1, 1)\n",
        "                images_crop_raw = []\n",
        "            else:\n",
        "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
        "                    image, min_num=2, max_num=9,\n",
        "                    image_size=self.image_size, use_thumbnail=False\n",
        "                )\n",
        "\n",
        "            # Process global view with padding\n",
        "            global_view = ImageOps.pad(\n",
        "                image, (self.base_size, self.base_size),\n",
        "                color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "            )\n",
        "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            width_crop_num, height_crop_num = crop_ratio\n",
        "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "\n",
        "            # Process local views (crops) if applicable\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                for crop_img in images_crop_raw:\n",
        "                    images_crop_list.append(\n",
        "                        self.image_transform(crop_img).to(self.dtype)\n",
        "                    )\n",
        "\n",
        "            # Calculate image tokens\n",
        "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                    num_queries * height_crop_num)\n",
        "\n",
        "        else:  # crop_mode = False\n",
        "            crop_ratio = (1, 1)\n",
        "            images_spatial_crop.append([1, 1])\n",
        "\n",
        "            # For smaller base sizes, resize; for larger, pad\n",
        "            if self.base_size <= 640:\n",
        "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
        "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
        "            else:\n",
        "                global_view = ImageOps.pad(\n",
        "                    image, (self.base_size, self.base_size),\n",
        "                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                )\n",
        "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
        "\n",
        "    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Process a single conversation into model inputs.\n",
        "            \"\"\"\n",
        "\n",
        "            # --- 1. Setup ---\n",
        "            images = []\n",
        "            for message in messages:\n",
        "                if \"images\" in message and message[\"images\"]:\n",
        "                    for img_data in message[\"images\"]:\n",
        "                        if img_data is not None:\n",
        "                            pil_image = self.deserialize_image(img_data)\n",
        "                            images.append(pil_image)\n",
        "\n",
        "            if not images:\n",
        "                raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n",
        "\n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list, images_crop_list, images_spatial_crop = [], [], []\n",
        "\n",
        "            prompt_token_count = -1 # Index to start training\n",
        "            assistant_started = False\n",
        "            image_idx = 0\n",
        "\n",
        "            # Add BOS token at the very beginning\n",
        "            tokenized_str.append(self.bos_id)\n",
        "            images_seq_mask.append(False)\n",
        "\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "\n",
        "                # Check if this is the assistant's turn\n",
        "                if role == \"<|Assistant|>\":\n",
        "                    if not assistant_started:\n",
        "                        # This is the split point. All tokens added *so far*\n",
        "                        # are part of the prompt.\n",
        "                        prompt_token_count = len(tokenized_str)\n",
        "                        assistant_started = True\n",
        "\n",
        "                    # Append the EOS token string to the *end* of assistant content\n",
        "                    content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Split this message's content by the image token\n",
        "                text_splits = content.split('<image>')\n",
        "\n",
        "                for i, text_sep in enumerate(text_splits):\n",
        "                    # Tokenize the text part\n",
        "                    tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
        "                    tokenized_str.extend(tokenized_sep)\n",
        "                    images_seq_mask.extend([False] * len(tokenized_sep))\n",
        "\n",
        "                    # If this text is followed by an <image> tag\n",
        "                    if i < len(text_splits) - 1:\n",
        "                        if image_idx >= len(images):\n",
        "                            raise ValueError(\n",
        "                                f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n",
        "                            )\n",
        "\n",
        "                        # Process the image\n",
        "                        image = images[image_idx]\n",
        "                        img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n",
        "\n",
        "                        images_list.extend(img_list)\n",
        "                        images_crop_list.extend(crop_list)\n",
        "                        images_spatial_crop.extend(spatial_crop)\n",
        "\n",
        "                        # Add image placeholder tokens\n",
        "                        tokenized_str.extend(tok_img)\n",
        "                        images_seq_mask.extend([True] * len(tok_img))\n",
        "\n",
        "                        image_idx += 1 # Move to the next image\n",
        "\n",
        "            # --- 3. Validation and Final Prep ---\n",
        "            if image_idx != len(images):\n",
        "                raise ValueError(\n",
        "                    f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n",
        "                )\n",
        "\n",
        "            # If we never found an assistant message, we're in a weird state\n",
        "            # (e.g., user-only prompt). We mask everything.\n",
        "            if not assistant_started:\n",
        "                print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n",
        "                prompt_token_count = len(tokenized_str)\n",
        "\n",
        "            # Prepare image tensors\n",
        "            images_ori = torch.stack(images_list, dim=0)\n",
        "            images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n",
        "\n",
        "            if images_crop_list:\n",
        "                images_crop = torch.stack(images_crop_list, dim=0)\n",
        "            else:\n",
        "                images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n",
        "                \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n",
        "                \"images_ori\": images_ori,\n",
        "                \"images_crop\": images_crop,\n",
        "                \"images_spatial_crop\": images_spatial_crop_tensor,\n",
        "                \"prompt_token_count\": prompt_token_count, # This is now accurate\n",
        "            }\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of samples\"\"\"\n",
        "        batch_data = []\n",
        "\n",
        "        # Process each sample\n",
        "        for feature in features:\n",
        "            try:\n",
        "                processed = self.process_single_sample(feature['messages'])\n",
        "                batch_data.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_data:\n",
        "            raise ValueError(\"No valid samples in batch\")\n",
        "\n",
        "        # Extract lists\n",
        "        input_ids_list = [item['input_ids'] for item in batch_data]\n",
        "        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n",
        "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens (model shouldn't predict these)\n",
        "        labels[images_seq_mask] = -100\n",
        "\n",
        "        # Mask user prompt tokens when train_on_responses_only=True (only train on assistant responses)\n",
        "        if self.train_on_responses_only:\n",
        "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
        "                if prompt_count > 0:\n",
        "                    labels[idx, :prompt_count] = -100\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        # Prepare images batch (list of tuples)\n",
        "        images_batch = []\n",
        "        for item in batch_data:\n",
        "            images_batch.append((item['images_crop'], item['images_ori']))\n",
        "\n",
        "        # Stack spatial crop info\n",
        "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask,\n",
        "            \"images_spatial_crop\": images_spatial_crop,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `DeepSeekOCRDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from unsloth import is_bf16_supported\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "data_collator = DeepSeekOCRDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    model = model,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    train_on_responses_only=True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        fp16 = not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        dataloader_num_workers = 2,\n",
        "        remove_unused_columns = False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([dataset[\"train\"][0]])\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "3phdO3Htsp21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = '0000_samples.png'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd30-Yg3fEeK"
      },
      "source": [
        "With only 60 steps, we dramatically improved the transcription quality. The Character Error Rate (CER) on this single sample dropped from 23% to 6%, a 74% relative reduction!\n",
        "\n",
        "| Type | OCR |\n",
        "| :--- | :--- |\n",
        "| **Baseline (Pre-Finetune)** | `انضباطم نندم حقيقتن باورم نميتند` |\n",
        "| **Finetuned (60 steps)** | `انضباطم نشدم حقیقتن باورم نمیشد` |\n",
        "| **Ground Truth** | `انضباطمم شدم حقیقتن باورم نمیشد` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "        auto_model = AutoModel,\n",
        "        trust_remote_code=True,\n",
        "        unsloth_force_compile=True,\n",
        "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "prompt = \"<image>\\nFree OCR. \"\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxZeNa2s1t48"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}